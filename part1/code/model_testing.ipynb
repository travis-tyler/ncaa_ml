{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NCAA ML contest - Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Season', 'DayNum', 'Team1', 'Team2', 'Tourney', 'WLoc', 'ScoreDiff',\n",
       "       'Team1Seed', 'Team2Seed', 'Team1FirstYear', 'Team1LastYear',\n",
       "       'Team2FirstYear', 'Team2LastYear', 'WTeam', 'Team1RankMean',\n",
       "       'Team2RankMean', 'WinCount_Team1', 'GameCount_Team1', 'AvgScore_Team1',\n",
       "       'Win%_Team1', 'WinCount_Team2', 'GameCount_Team2', 'AvgScore_Team2',\n",
       "       'Win%_Team2', 'LoseCount_Team1', 'AvgFGM_Team1', 'LoseCount_Team2',\n",
       "       'AvgFGM_Team2', 'AvgFGA_Team1', 'AvgFGA_Team2', 'AvgFGM3_Team1',\n",
       "       'AvgFGM3_Team2', 'AvgFGA3_Team1', 'AvgFGA3_Team2', 'AvgFTM_Team1',\n",
       "       'AvgFTM_Team2', 'AvgFTA_Team1', 'AvgFTA_Team2', 'AvgOR_Team1',\n",
       "       'AvgOR_Team2', 'AvgDR_Team1', 'AvgDR_Team2', 'AvgAst_Team1',\n",
       "       'AvgAst_Team2', 'AvgTO_Team1', 'AvgTO_Team2', 'AvgStl_Team1',\n",
       "       'AvgStl_Team2', 'AvgBlk_Team1', 'AvgBlk_Team2', 'AvgPF_Team1',\n",
       "       'AvgPF_Team2', 'FG%_Team1', 'FG%_Team2', 'FG3%_Team1', 'FG3%_Team2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in data\n",
    "all_stats_df = pd.DataFrame(pd.read_csv('../resources/transformed_data/all_stats.csv'))\n",
    "all_stats_df = all_stats_df.drop(columns=['WTeamID'])\n",
    "all_stats_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93200, 14)\n",
      "(93200,)\n"
     ]
    }
   ],
   "source": [
    "# easy way to comment out unwanted features\n",
    "features_df = all_stats_df[['Season', \n",
    "#                             'DayNum', \n",
    "                            'Team1', 'Team2', \n",
    "#                             'Tourney', \n",
    "                            'WLoc', \n",
    "#                             'ScoreDiff',\n",
    "                            'Team1RankMean', 'Team2RankMean',\n",
    "                            'Team1Seed', 'Team2Seed',\n",
    "#                             'Team1FirstYear', 'Team2FirstYear',\n",
    "#                             'Team1LastYear', 'Team2LastYear',\n",
    "#                             'WinCount_Team1', 'WinCount_Team2',\n",
    "                            'LoseCount_Team1', 'LoseCount_Team2',\n",
    "                            'GameCount_Team1', 'GameCount_Team2',\n",
    "                            'Win%_Team1', 'Win%_Team2',\n",
    "#                             'AvgScore_Team1', 'AvgScore_Team2',\n",
    "#                             'AvgFGM_Team1', 'AvgFGM_Team2', \n",
    "#                             'AvgFGA_Team1', 'AvgFGA_Team2',\n",
    "#                             'AvgFGM3_Team1', 'AvgFGM3_Team2', \n",
    "#                             'AvgFGA3_Team1', 'AvgFGA3_Team2',\n",
    "#                             'AvgFTM_Team1', 'AvgFTM_Team2', \n",
    "#                             'AvgFTA_Team1', 'AvgFTA_Team2',\n",
    "#                             'AvgOR_Team1', 'AvgOR_Team2', \n",
    "#                             'AvgDR_Team1', 'AvgDR_Team2',\n",
    "#                             'AvgAst_Team1', 'AvgAst_Team2', \n",
    "#                             'AvgTO_Team1', 'AvgTO_Team2',\n",
    "#                             'AvgStl_Team1', 'AvgStl_Team2', \n",
    "#                             'AvgBlk_Team1', 'AvgBlk_Team2',\n",
    "#                             'AvgPF_Team1', 'AvgPF_Team2',\n",
    "#                             'FG%_Team1', 'FG%_Team2',\n",
    "#                             'FG3%_Team1', 'FG3%_Team2',\n",
    "                            'WTeam',\n",
    "                           ]] \n",
    "\n",
    "# drop all NaNs\n",
    "features_df = features_df.dropna(how='any')\n",
    "# drop overcorrected rank \n",
    "features_df = features_df.loc[features_df.Team1RankMean < 500]\n",
    "features_df = features_df.loc[features_df.Team2RankMean < 500]\n",
    "\n",
    "# # shuffle and limit dataset (for time-consuming model training)\n",
    "# features_df = features_df.sample(frac=1).reset_index(drop=True)[:50000]\n",
    "# features_df = features_df.sample(frac=1).reset_index(drop=True)[:50000]\n",
    "\n",
    "\n",
    "# # change column type to test one hot encoder and scaler\n",
    "# features_df = features_df.astype({\n",
    "#                                   'Season':'str',\n",
    "#                                   'Team1':'str',\n",
    "#                                   'Team2':'str',\n",
    "#                                  })\n",
    "\n",
    "# select features and target\n",
    "target = features_df.pop('WTeam')\n",
    "selected_features = features_df\n",
    "print(selected_features.shape)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.70146472, -1.40911253, -0.1899856 , ..., -0.09400181,\n",
       "         0.44542821,  1.8236377 ],\n",
       "       [-1.70146472,  0.54719759,  0.56965234, ..., -0.55951554,\n",
       "         1.80511893,  2.05889961],\n",
       "       [-1.70146472,  0.47732938,  1.08386878, ..., -0.09400181,\n",
       "         2.04391834, -0.21988858],\n",
       "       ...,\n",
       "       [ 0.44899565, -0.72207505, -1.73263494, ...,  1.76805313,\n",
       "         1.57411988,  3.11165213],\n",
       "       [ 0.44899565,  0.24443531,  1.32929027, ...,  1.30253939,\n",
       "         1.12241275,  1.72278698],\n",
       "       [ 0.44899565, -0.72207505, -1.14829807, ...,  1.76805313,\n",
       "         1.57411988,  1.08099875]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One hot encoder and scaler for pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.compose import make_column_selector\n",
    "\n",
    "ct = make_column_transformer(\n",
    "    (StandardScaler(), make_column_selector(dtype_include=np.number)),\n",
    "    (OneHotEncoder(handle_unknown='ignore'), make_column_selector(dtype_include=object))\n",
    ")\n",
    "\n",
    "ct.fit_transform(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.model_selection import GridSearchCV, LeaveOneOut, RandomizedSearchCV\n",
    "# from sklearn import metrics\n",
    "\n",
    "# os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "# mlc = MLPClassifier(activation = 'relu', random_state=1, nesterovs_momentum=True)\n",
    "# loo = LeaveOneOut()\n",
    "# pipe = make_pipeline(ct, mlc)\n",
    "\n",
    "# params = {\n",
    "#           \"mlpclassifier__hidden_layer_sizes\":[(168,),(126,),(498,),(166,)],\n",
    "#           \"mlpclassifier__solver\" : ('sgd','adam'), \n",
    "#           \"mlpclassifier__alpha\": [0.001,0.0001],\n",
    "#           \"mlpclassifier__learning_rate_init\":[0.005,0.001]\n",
    "#          }\n",
    "\n",
    "# clf = RandomizedSearchCV(pipe, params, n_jobs=-1, verbose=3)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(selected_features, \n",
    "#                                                     target, stratify=target, \n",
    "#                                                     random_state=42)\n",
    "\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# model = clf.best_estimator_\n",
    "# print(\"the best model and parameters are the following: {} \".format(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best model: \n",
    "\n",
    "MLPClassifier(alpha=0.001, hidden_layer_sizes=(168,),\n",
    "              learning_rate_init=0.005, random_state=1))]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLPClassifier(alpha=0.001, hidden_layer_sizes=(168,), random_state=1, solver='sgd')\n",
    " \n",
    "Test score: 0.7595148169668797\n",
    "\n",
    "Log loss: 0.8306158860092259"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------\n",
      "Test score: 0.7608154506437769\n",
      "Log loss: 8.261239575120412\n",
      "\n",
      "-------------------\n",
      "\n",
      "[[9128 2853]\n",
      " [2720 8599]]\n",
      "\n",
      "-------------------\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.76      0.77     11981\n",
      "           1       0.75      0.76      0.76     11319\n",
      "\n",
      "    accuracy                           0.76     23300\n",
      "   macro avg       0.76      0.76      0.76     23300\n",
      "weighted avg       0.76      0.76      0.76     23300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV, LeaveOneOut\n",
    "from sklearn import metrics\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(selected_features, \n",
    "                                                    target, \n",
    "                                                    random_state=42)\n",
    "\n",
    "mlc = MLPClassifier(alpha=0.01, hidden_layer_sizes=(16,16), \n",
    "                    random_state=1, \n",
    "                    solver='sgd', \n",
    "#                     learning_rate='invscaling', \n",
    "#                     learning_rate_init=0.01,\n",
    "#                     power_t=.1,\n",
    "#                     max_iter=200, \n",
    "                    nesterovs_momentum=True, \n",
    "#                     batch_size=75,\n",
    "                    verbose=True)\n",
    "\n",
    "# mlc = MLPClassifier(alpha=0.001, hidden_layer_sizes=(14,14), \n",
    "#                     random_state=1, solver='sgd', learning_rate_init=0.01,)\n",
    "\n",
    "\n",
    "pipe = make_pipeline(ct, mlc)\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "pipe.predict_proba(X_test)\n",
    "preds = pipe.predict(X_test)\n",
    "train_score = pipe.score(X_train, y_train)\n",
    "test_score = pipe.score(X_test, y_test)\n",
    "loss_score = metrics.log_loss(y_test, preds)\n",
    "\n",
    "print('\\n-------------------')\n",
    "print(f'Test score: {test_score}')\n",
    "print(f'Log loss: {loss_score}')\n",
    "print('\\n-------------------\\n')\n",
    "print(metrics.confusion_matrix(y_test,preds))\n",
    "print('\\n-------------------\\n')\n",
    "print(metrics.classification_report(y_test,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_submit(model, df):\n",
    "    '''Creates and formats submission '''\n",
    "    preds = model.predict(df)\n",
    "    prob = [x[1] for x in model.predict_proba(df)]\n",
    "    \n",
    "    predict_df = pd.DataFrame({'Season':df['Season'],\n",
    "                  'Team1':df['Team1'],\n",
    "                  'Team2':df['Team2'],\n",
    "                  'Guess':preds,\n",
    "                  'Pred':prob}).round(3)\n",
    "\n",
    "    predict_df['ID'] = predict_df['Season'].astype(str) + '_' +\\\n",
    "                       predict_df['Team1'].astype(str) + '_' +\\\n",
    "                       predict_df['Team2'].astype(str)\n",
    "    \n",
    "    submit_df = predict_df[['ID','Pred']].set_index('ID')\n",
    "    \n",
    "#     submit_df['Pred'] = np.where(submit_df['Pred']<0.1, submit_df['Pred']-.1, submit_df['Pred'])\n",
    "#     submit_df['Pred'] = np.where(submit_df['Pred']>.9, submit_df['Pred']+.1, submit_df['Pred'])\n",
    "\n",
    "    submit_df.to_csv('../output/ml_march_madness_submission.csv')\n",
    "    \n",
    "    return submit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Season', 'Team1', 'Team2', 'Tourney', 'WLoc', 'Team1Seed', 'Team2Seed',\n",
       "       'Team1FirstYear', 'Team1LastYear', 'Team2FirstYear', 'Team2LastYear',\n",
       "       'Team1RankMean', 'Team2RankMean', 'WinCount_Team1', 'GameCount_Team1',\n",
       "       'AvgScore_Team1', 'Win%_Team1', 'WinCount_Team2', 'GameCount_Team2',\n",
       "       'AvgScore_Team2', 'Win%_Team2', 'LoseCount_Team1', 'AvgFGM_Team1',\n",
       "       'LoseCount_Team2', 'AvgFGM_Team2', 'AvgFGA_Team1', 'AvgFGA_Team2',\n",
       "       'AvgFGM3_Team1', 'AvgFGM3_Team2', 'AvgFGA3_Team1', 'AvgFGA3_Team2',\n",
       "       'AvgFTM_Team1', 'AvgFTM_Team2', 'AvgFTA_Team1', 'AvgFTA_Team2',\n",
       "       'AvgOR_Team1', 'AvgOR_Team2', 'AvgDR_Team1', 'AvgDR_Team2',\n",
       "       'AvgAst_Team1', 'AvgAst_Team2', 'AvgTO_Team1', 'AvgTO_Team2',\n",
       "       'AvgStl_Team1', 'AvgStl_Team2', 'AvgBlk_Team1', 'AvgBlk_Team2',\n",
       "       'AvgPF_Team1', 'AvgPF_Team2', 'FG%_Team1', 'FG%_Team2', 'FG3%_Team1',\n",
       "       'FG3%_Team2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_stats_submit_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11390, 14)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_stats_submit_df = pd.DataFrame(pd.read_csv('../resources/transformed_data/all_stats_submit.csv'))\n",
    "submit_df = all_stats_submit_df[selected_features.columns]\n",
    "submit_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015_1107_1112</th>\n",
       "      <td>0.224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015_1107_1116</th>\n",
       "      <td>0.204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015_1107_1124</th>\n",
       "      <td>0.233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015_1107_1125</th>\n",
       "      <td>0.320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015_1107_1129</th>\n",
       "      <td>0.207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019_1449_1459</th>\n",
       "      <td>0.665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019_1449_1463</th>\n",
       "      <td>0.731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019_1458_1459</th>\n",
       "      <td>0.719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019_1458_1463</th>\n",
       "      <td>0.776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019_1459_1463</th>\n",
       "      <td>0.607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11390 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Pred\n",
       "ID                   \n",
       "2015_1107_1112  0.224\n",
       "2015_1107_1116  0.204\n",
       "2015_1107_1124  0.233\n",
       "2015_1107_1125  0.320\n",
       "2015_1107_1129  0.207\n",
       "...               ...\n",
       "2019_1449_1459  0.665\n",
       "2019_1449_1463  0.731\n",
       "2019_1458_1459  0.719\n",
       "2019_1458_1463  0.776\n",
       "2019_1459_1463  0.607\n",
       "\n",
       "[11390 rows x 1 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_submit(pipe, submit_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(transformers=[('standardscaler',\n",
       "                                                  StandardScaler(),\n",
       "                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x7fc9093e7590>),\n",
       "                                                 ('onehotencoder',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x7fc9093e77d0>)])),\n",
       "                ('mlpclassifier',\n",
       "                 MLPClassifier(alpha=0.001, hidden_layer_sizes=(16, 16),\n",
       "                               learning_rate_init=0.01, random_state=1,\n",
       "                               solver='sgd'))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# filename = '../models/mlp_model_1.sav'\n",
    "# pickle.dump(pipe, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "# from sklearn.gaussian_process.kernels import RBF\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn import metrics\n",
    "# import os\n",
    "\n",
    "# os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(selected_features, \n",
    "#                                                     target, \n",
    "#                                                     random_state=42)\n",
    "\n",
    "# # clf = GaussianProcessClassifier(1.0 * RBF(1.0))\n",
    "# clf = GaussianProcessClassifier(n_jobs=-1)\n",
    "# pipe = make_pipeline(ct, clf)\n",
    "# pipe.fit(X_train, y_train)\n",
    "\n",
    "# pipe.predict_proba(X_test)\n",
    "# preds = pipe.predict(X_test)\n",
    "# train_score = pipe.score(X_train, y_train)\n",
    "# test_score = pipe.score(X_test, y_test)\n",
    "# loss_score = metrics.log_loss(y_test, preds)*.1\n",
    "\n",
    "# print(f'Train score: {test_score}')\n",
    "# print(f'Test score: {test_score}')\n",
    "# print(f'Log loss: {loss_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best log loss: Log loss: 0.8206507730540533"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV, LeaveOneOut\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(selected_features, \n",
    "#                                                     target, \n",
    "#                                                     random_state=42)\n",
    "\n",
    "# rfc = RandomForestClassifier(n_jobs=-1, max_features= 'sqrt', n_estimators=50, oob_score = True) \n",
    "\n",
    "# pipe = make_pipeline(ct, rfc)\n",
    "\n",
    "# param_grid = {\n",
    "#     'randomforestclassifier__n_estimators': [100, 500, 1000],\n",
    "#     'randomforestclassifier__max_features': ['log2', 'sqrt','auto'],\n",
    "# }\n",
    "\n",
    "# CV_rfc = GridSearchCV(pipe, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1000)\n",
    "# CV_rfc.fit(X_train, y_train)\n",
    "# print(CV_rfc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rfc = RandomForestClassifier(n_jobs=-1, max_features='log2', n_estimators=500, oob_score = True) \n",
    "\n",
    "# pipe = make_pipeline(ct, rfc)\n",
    "# pipe.fit(X_train, y_train)\n",
    "\n",
    "# pipe.predict_proba(X_test)\n",
    "# preds = pipe.predict(X_test)\n",
    "# train_score = pipe.score(X_train, y_train)\n",
    "# test_score = pipe.score(X_test, y_test)\n",
    "# loss_score = metrics.log_loss(y_test, preds)*.1\n",
    "\n",
    "# print(f'Train score: {test_score}')\n",
    "# print(f'Test score: {test_score}')\n",
    "# print(f'Log loss: {loss_score}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
